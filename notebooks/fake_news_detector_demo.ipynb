{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c324d5",
   "metadata": {},
   "source": [
    "# üîç Real-Time Fake News Detector - Experimentation Notebook\n",
    "\n",
    "This notebook demonstrates how to use the fake news detection system and experiment with different approaches.\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "First, make sure you have installed all dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75cbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run this cell if packages are not installed)\n",
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "from preprocessing import TextPreprocessor, preprocess_dataset\n",
    "from train_model import FakeNewsClassifier, create_sample_dataset\n",
    "from predict import FakeNewsDetector, quick_predict\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11807e03",
   "metadata": {},
   "source": [
    "## 1. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset for experimentation\n",
    "print(\"üîÑ Creating sample dataset...\")\n",
    "df = create_sample_dataset()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Display first few examples\n",
    "print(f\"\\nüì∞ Sample articles:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels = ['Fake', 'Real']\n",
    "counts = df['label'].value_counts().sort_index()\n",
    "colors = ['#ff6b6b', '#4ecdc4']\n",
    "\n",
    "plt.pie(counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Fake vs Real News Articles')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='label', palette=colors)\n",
    "plt.title('Count of Fake vs Real News Articles')\n",
    "plt.xlabel('Label (0=Fake, 1=Real)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Fake', 'Real'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore text preprocessing\n",
    "print(\"üîß Testing text preprocessing...\")\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = TextPreprocessor(remove_stopwords=True, use_stemming=False)\n",
    "\n",
    "# Test on sample text\n",
    "sample_text = \"\"\"<h1>BREAKING NEWS!!!</h1> \n",
    "Scientists have DISCOVERED that aliens are living among us!!! \n",
    "Visit https://fakenews.com for more details. Contact us at info@fake.com!!!\"\"\"\n",
    "\n",
    "cleaned_text = preprocessor.clean_text(sample_text)\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nCleaned text:\")\n",
    "print(cleaned_text)\n",
    "\n",
    "# Preprocess the entire dataset\n",
    "df_processed = preprocess_dataset(df, 'text', preprocessor)\n",
    "print(f\"\\n‚úÖ Dataset preprocessed successfully!\")\n",
    "print(f\"Original text length (avg): {df['text'].str.len().mean():.1f} characters\")\n",
    "print(f\"Processed text length (avg): {df_processed['text'].str.len().mean():.1f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cac5ff",
   "metadata": {},
   "source": [
    "## 2. Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train different models and compare performance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"ü§ñ Training and comparing different models...\")\n",
    "\n",
    "# Prepare data\n",
    "X = df['text'].tolist()\n",
    "y = df['label'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Train models\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "for model_type in ['logistic', 'random_forest']:\n",
    "    print(f\"\\n{'='*20} Training {model_type.upper()} {'='*20}\")\n",
    "    \n",
    "    # Initialize and train\n",
    "    classifier = FakeNewsClassifier(model_type=model_type, use_preprocessing=True)\n",
    "    classifier.train(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Store model\n",
    "    models[model_type] = classifier\n",
    "    \n",
    "    # Evaluate\n",
    "    predictions = classifier.predict(X_test)\n",
    "    accuracy = classifier.evaluate(X_test, y_test)\n",
    "    results[model_type] = accuracy\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"üìä Model Comparison:\")\n",
    "for model_type, accuracy in results.items():\n",
    "    print(f\"{model_type.upper()}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f46aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "accuracies = list(results.values())\n",
    "\n",
    "bars = plt.bar(model_names, accuracies, color=['#3498db', '#e74c3c'], alpha=0.8)\n",
    "plt.title('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Model Type', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, accuracy in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{accuracy:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bded57",
   "metadata": {},
   "source": [
    "## 3. Detailed Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bdc714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the best performing model in detail\n",
    "best_model_type = max(results, key=results.get)\n",
    "best_model = models[best_model_type]\n",
    "\n",
    "print(f\"üìà Analyzing best model: {best_model_type.upper()}\")\n",
    "print(f\"Best accuracy: {results[best_model_type]:.4f}\")\n",
    "\n",
    "# Get predictions for confusion matrix\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_prob = best_model.predict_proba(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.title(f'Confusion Matrix - {best_model_type.upper()} Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed classification report\n",
    "print(f\"\\nüìã Detailed Classification Report:\")\n",
    "target_names = ['Fake', 'Real']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence distribution\n",
    "confidence_scores = [max(prob) for prob in y_prob]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Confidence distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(confidence_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Prediction Confidence')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(np.mean(confidence_scores), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(confidence_scores):.3f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Confidence by prediction correctness\n",
    "plt.subplot(1, 2, 2)\n",
    "correct_predictions = (y_test == y_pred)\n",
    "correct_confidence = [conf for conf, correct in zip(confidence_scores, correct_predictions) if correct]\n",
    "incorrect_confidence = [conf for conf, correct in zip(confidence_scores, correct_predictions) if not correct]\n",
    "\n",
    "plt.hist(correct_confidence, bins=15, alpha=0.7, label='Correct Predictions', color='green')\n",
    "plt.hist(incorrect_confidence, bins=15, alpha=0.7, label='Incorrect Predictions', color='red')\n",
    "plt.title('Confidence by Prediction Correctness')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average confidence for correct predictions: {np.mean(correct_confidence):.3f}\")\n",
    "print(f\"Average confidence for incorrect predictions: {np.mean(incorrect_confidence):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc8dad",
   "metadata": {},
   "source": [
    "## 4. Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2592d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model for use in prediction module\n",
    "print(f\"üíæ Saving the best model ({best_model_type})...\")\n",
    "best_model.save_model('../models')\n",
    "print(\"‚úÖ Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde616f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom examples\n",
    "def test_custom_news(text, model_type='logistic'):\n",
    "    \"\"\"Test a custom news article.\"\"\"\n",
    "    detector = FakeNewsDetector(model_type=model_type)\n",
    "    if detector.is_loaded:\n",
    "        result = detector.predict(text)\n",
    "        \n",
    "        # Format output\n",
    "        prediction = result['prediction']\n",
    "        confidence = result['confidence']\n",
    "        emoji = \"‚úÖ\" if prediction == \"REAL\" else \"‚ùå\"\n",
    "        \n",
    "        print(f\"üì∞ Article: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
    "        print(f\"{emoji} Prediction: {prediction} (Confidence: {confidence}%)\")\n",
    "        print(f\"üìä Probabilities: Fake={result['probability_fake']:.3f}, Real={result['probability_real']:.3f}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "        print(\"‚ùå Model not loaded. Please train the model first.\")\n",
    "        return None\n",
    "\n",
    "# Test examples\n",
    "test_articles = [\n",
    "    \"Local university receives federal grant to study climate change impacts on agriculture\",\n",
    "    \"Scientists discover aliens living on Mars and planning invasion of Earth next year\",\n",
    "    \"Stock market closes up 3% following positive quarterly earnings reports from tech sector\",\n",
    "    \"Miracle weight loss pill discovered that allows you to lose 50 pounds in one week\",\n",
    "    \"City council approves new budget allocation for public transportation improvements\",\n",
    "    \"Government admits to hiding cure for cancer for past 30 years to protect pharmaceutical profits\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing with sample articles:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for article in test_articles:\n",
    "    test_custom_news(article, best_model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4fabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing cell - modify this to test your own articles\n",
    "custom_article = \"\"\"\n",
    "Breaking: Scientists at a major university have announced a breakthrough in renewable energy \n",
    "technology that could revolutionize how we power our homes and businesses. The new solar panel \n",
    "design increases efficiency by 40% while reducing manufacturing costs by 25%. The research team \n",
    "plans to begin commercial production within the next two years.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"üîç Testing your custom article:\")\n",
    "print(\"=\" * 80)\n",
    "test_custom_news(custom_article, best_model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893ad89",
   "metadata": {},
   "source": [
    "## 5. Feature Analysis (Advanced)\n",
    "\n",
    "Let's analyze what features the model considers important for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13deee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze most important features (for logistic regression)\n",
    "if best_model_type == 'logistic':\n",
    "    print(\"üîç Analyzing most important features for classification...\")\n",
    "    \n",
    "    # Get feature names and coefficients\n",
    "    feature_names = best_model.vectorizer.get_feature_names_out()\n",
    "    coefficients = best_model.model.coef_[0]\n",
    "    \n",
    "    # Sort by absolute coefficient value\n",
    "    feature_importance = list(zip(feature_names, coefficients))\n",
    "    feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    # Top features for fake news (negative coefficients)\n",
    "    fake_features = [(name, coef) for name, coef in feature_importance if coef < 0][:10]\n",
    "    # Top features for real news (positive coefficients)\n",
    "    real_features = [(name, coef) for name, coef in feature_importance if coef > 0][:10]\n",
    "    \n",
    "    print(\"\\nüì∞ Top words/phrases indicating FAKE news:\")\n",
    "    for i, (feature, coef) in enumerate(fake_features, 1):\n",
    "        print(f\"{i:2d}. {feature:<20} (coefficient: {coef:.4f})\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Top words/phrases indicating REAL news:\")\n",
    "    for i, (feature, coef) in enumerate(real_features, 1):\n",
    "        print(f\"{i:2d}. {feature:<20} (coefficient: {coef:.4f})\")\n",
    "    \n",
    "    # Visualize top features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Combine top fake and real features for plotting\n",
    "    plot_features = fake_features[-5:] + real_features[:5]  # 5 from each\n",
    "    feature_names_plot = [f[0] for f in plot_features]\n",
    "    coefficients_plot = [f[1] for f in plot_features]\n",
    "    colors = ['red' if c < 0 else 'green' for c in coefficients_plot]\n",
    "    \n",
    "    plt.barh(range(len(feature_names_plot)), coefficients_plot, color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(feature_names_plot)), feature_names_plot)\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title('Most Important Features for Fake News Classification')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.text(0.02, 0.98, 'Red: Indicates FAKE news', transform=plt.gca().transAxes, \n",
    "             verticalalignment='top', color='red', fontweight='bold')\n",
    "    plt.text(0.02, 0.93, 'Green: Indicates REAL news', transform=plt.gca().transAxes, \n",
    "             verticalalignment='top', color='green', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Feature importance analysis is currently only available for logistic regression models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3764b0",
   "metadata": {},
   "source": [
    "## 6. Model Deployment Ready\n",
    "\n",
    "Your model is now ready for deployment! Here's what you can do next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a08c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Your Fake News Detector is ready for deployment!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Model Performance Summary:\")\n",
    "for model_type, accuracy in results.items():\n",
    "    print(f\"  ‚Ä¢ {model_type.upper()}: {accuracy:.1%} accuracy\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_type.upper()} ({results[best_model_type]:.1%} accuracy)\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"1. üåê Start the Flask API:\")\n",
    "print(\"   cd ../api && python app.py\")\n",
    "print(\"   Then visit: http://localhost:5000\")\n",
    "\n",
    "print(\"\\n2. üìö Get a real dataset:\")\n",
    "print(\"   ‚Ä¢ Download from Kaggle: 'Fake News Dataset'\")\n",
    "print(\"   ‚Ä¢ Place CSV in ../data/ folder\")\n",
    "print(\"   ‚Ä¢ Retrain with: python ../src/train_model.py\")\n",
    "\n",
    "print(\"\\n3. üîß Improve the model:\")\n",
    "print(\"   ‚Ä¢ Try different preprocessing options\")\n",
    "print(\"   ‚Ä¢ Experiment with BERT/RoBERTa models\")\n",
    "print(\"   ‚Ä¢ Add more features (source credibility, etc.)\")\n",
    "\n",
    "print(\"\\n4. üöÄ Deploy to production:\")\n",
    "print(\"   ‚Ä¢ Containerize with Docker\")\n",
    "print(\"   ‚Ä¢ Deploy to cloud (AWS, GCP, Azure)\")\n",
    "print(\"   ‚Ä¢ Add monitoring and logging\")\n",
    "\n",
    "print(\"\\n‚ú® Happy fake news detecting!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
